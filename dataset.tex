The dataset is taken from the British newspaper \textit{The Guardian} that contains approximately $626$K article URLs with $61.5$M corresponding comments from $1.25$M authors.
The comment data provided us with the comment text itself, the article reference, the author, the creation timestamp, a reference of the parent comment, and the number of upvotes. Additionally, we had the comment author's username and a dedicated reference.

The comments were posted between 2006 and 2017. All $1.25$M users wrote at least one comment, $22$\% of them more than $10$, and $6$\% over $100$.
The given articles are published in one out of $79$ categories. 
Articles featuring contentions issues are published in the \textit{comment is free} category, accounting $18$\% of all articles released.
Consequently, this category is also the most commented one. 
Each of the others, e.g.\ \textit{sport}, \textit{music}, and \textit{politics} cover less than $7$\% of the given dataset.

\subsection{Data Cleaning}
Analysis of the dataset revealed that the distribution of the number of comments per article exhibited an anomalous peak at exactly $50$ comments. 
Therefore, these articles were excluded which reduced our dataset size by about $2$\%.

To avoid anomalies using features that we derived from the article's publication time, we removed a small number of articles that were released on the 29th of February.

\subsection{Data Enrichment}
To enrich our dataset, we used the official \textit{Guardian API} \footnote{\url{https://open-platform.theguardian.com}} which provided the article text and further attributes such as the category, the headline, and the publication date.
Due to API restrictions, it was not possible to download all the articles hence why the number of articles was reduced by about $11$\%.
Furthermore, we extracted metadata such as the headline and article word count, and derived time features such as the day of the year, the day of the week, hour, and minute from the publication date.

Articles released at a similar time and discussing a similar topic will share their comment volume. To take this into account, a \textit{competitive score} was developed as an additional feature:

\begin{equation} \label{eq:competitive_score}
	compet_i = \sum_{n=1}^{j} \frac{\sigma'(t_i - t_j)}{\norm{\overrightarrow{a_i} - \overrightarrow{{a_j}}}^2}, i \neq j
\end{equation}

\begin{flalign*}
	\sigma&: \text{sigmoid function} & \\
	t_i&: \text{publication time of article } i & \\
	\overrightarrow{a_i}&: \text{\textit{Doc2Vec} vector of article } i \text{ text}& \\
\end{flalign*}

The numerical value of $compet_i$ describes how much a given article $i$ competes with all other articles.
Thereby, the difference between the publication time of the given articles shows how close they are published to each other.
In practice, articles are often published at the same time which would result in a time difference of zero.
Application of the derivate of the sigmoid function in the numerator accounts for this.
In order to identify articles concerning similar topics, the \textit{Doc2Vec} algorithm \cite{le2014doc2vec} was trained on our article corpus and the Euclidean distance is used to calculate how similar articles are to each other.
The distance in the denominator is squared to increase the impact of the article similarity.

To decide if a given article was in the top $10$\% of the most commented on articles within its released week was determined by using the number of comments for that article.

Throughout this paper, this cleaned and enriched dataset is referred to as the Guardian Article and Comment Corpus (GACC) which contains approximately $547$K articles with $58.6$M comments.
