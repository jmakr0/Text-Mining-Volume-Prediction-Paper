Our work is based on a dataset of the British newspaper \textit{The Guardian} that contains approximately $626$K article URLs with $61.5$M corresponding comments from $1.25$M authors. 
The comment data provided us with the comment text itself, the author's reference, the timestamp when the comment was posted, a reference of the parent comment, and the number of upvotes. Additionally, we had the comment author's username and a dedicated reference. 

The comments were posted between 2006 and 2017. All $1.25$M users wrote at least one comment, $22$\% of them more than $10$, and $6$\% over $100$.
The given articles are published in one out of $79$ categories. In total, $18$\% of all released articles are published under the category \textit{comment is free} that is used to debate contentious issues. Consequently, this category is also the most commented one. 
Each of the others, e.g. \textit{sport}, \textit{music}, and \textit{politics} covers less than $7$\% in the given dataset.

We assume that the number of comments of a given article is an essential feature to predict the comment volume of similar articles in the future. Fortunately, this feature could be determined by simply counting the corresponding comments of each article. Afterwards, we were able to decide if a given article was in the top $10$\% of the most commented articles within its released week.

\subsection{Data Cleaning}
Analysing the dataset, we noticed that the distribution of the number of comments exhibited a bizarre peak at exactly $50$ comments per article. Therefore, we did not take these articles into account which reduced our dataset about $2$\% of the given articles.

To avoid anomalies using features that we derived from the articles, e.g. the publication time, we removed all articles that were released on 29th of February.

\subsection{Data Enrichment}
To enrich our dataset, we used the offical \textit{Guardian API} to get the article text and further attributes like the category, the headline, and the publication time.
Due to API restrictions we were not able to download all articles which is why the amount of articles was reduced about $11$\%.
Furthermore, we extracted metadata like the headline and article word count, the time data describing the day of the week, the day of the year, the hour and the minute of the publication date.

We suppose that articles that are released at a similar time and discussing a related topic are likely to share the amount of comments. To take this behaviour into account, we've developed a \textit{competitive score} (\ref{eq:1}) as an additional feature.

\begin{equation} \label{eq:1}
	compet_i = \sum_{n=1}^{j} \frac{\sigma'(t_i - t_j)}{\norm{\overrightarrow{a_i} - \overrightarrow{{a_j}}}^2}, i \neq j
\end{equation}

\begin{flalign*}
	\sigma&: \text{sigmoid function} & \\
	t_i&: \text{publication date of article } i & \\
	\overrightarrow{a_i}&: \text{doc2vec vector of article } i \text{ text}& \\
\end{flalign*}

The numerical value of $compet_i$ describes how many competitive articles exist to the given article $i$. Thereby, the difference of the publication time of the given articles determines how close they are published to each other. For the calculation of this feature, we've considered all articles that were released within a timespan of $\pm3$h.
In pratice, articles will be often published at the same time which implies that the time difference can be zero. This is also the reason why we apply the derivate of the sigmoid function in the numerator.
To decide if articles are discussing a similar topic, we've trained \textit{doc2vec} DOC2VEC-REFERENCE on our article corpus and use the euclidean distance to calculate how similar articles to each other are. We square the distance in the denominator to increase the impact of the article similarity.

In the following, we call the cleaned and enriched dataset the Guardian Article and Comment Corpus (GACC). Finally, the GACC contains approximately $547$K articles with $58.6$M comments.