We use \textit{precision} and \textit{recall} to evaluate our models. 
In practice, each article which produces a high amount of comments is more likely to bind resources of comment moderators and editors. Therefore, a high classification precision is important as well as a high recall to omit as few as possible.
Precision and recall are equally relevant and consequently, we use the harmonic mean, the $F_1$-score, of both metrics. As \textit{precision}, \textit{recall}, and the $F_1$-score are common metrics for binary classification problems, we are enabled to compare our results against existing approaches.

\subsection{Model Performances}
The overview of our results in \autoref{tbl:results_basic} depicts that all our models reach a significantly higher \textit{recall} than \textit{precision}.

\textit{Model 3} reaches the highest \textit{precision} with a value of $0.228$, and \textit{Model 7} the highest \textit{recall} with $0.917$. \textit{Model 4} is the best overall performing model with an $F_1$-score of $0.320$. 
%\textit{Model 1} to \textit{3} also performs relatively well with an $F_1$-scores of over $0.3$.

\textit{Model 1} and \textit{Model 2} process the same input. Despite the fact that the second model has approximately a third of the weights in its hidden layers, both models exhibit a similar performance for all three metrics.
We assume that the convolutional network can learn fewer features but has a much higher generalization potential through its shared weights.

\textit{Model 3} takes the first $50$ words of the article text as input. We discovered that using more or even all words has no positive effect on the model's predictions.

Compared to \textit{Model 1} and \textit{2} it reaches a worse \textit{recall} but a slightly higher \textit{precision} which leads to a similar $F_1$-score.
We suppose this occurs because of a similar context complexity which is provided by the headline and the first words of the article.

\textit{Model 4} achieves the best results. This seems surprising due to the low complexity of this feature but the relatively good results can be justified with the distribution of the categories within the GACC. 

FIGURE?

Articles assigned to the most used category have a probability of $22.8\%$ to be a top article. Classifying just these articles as a top article already leads to an $F_1$-score of $0.292$. If we only use the four categories with the highest percentage of top articles, leads to an $F_1$-score of $0.320$ which is the same result as achieved by this model.


\textit{Model 5} is the worst performing model. Its precision of $0.100$ is just as good as a random prediction.

FIGURE?

One reason is that $19$\% of all articles got released on full hours and might also be due to the fact that \textit{The Guardian} has an international readership spread over multiple time zones. Additionally, the day of the week has no significant effect on the weekly article performance. Therefore, this feature has almost no effect on the number of comments that an article receives. 

\textit{Model 6} had also a small but slightly better performance than \textit{Model 5}. 
Either the length of the headline or the length of the article text is feasible for an accurate prediction.

\textit{Model 7} processes just a single numeric value as input. 

For a good performance using the competitive score it would be necessary to be able to divide the values into distinct intervals that can be assigned to each class.
Unfortunately, the competitive score distribution of top articles is similar to the overall distribution of the competitive score.

\input{basic_results.tex}

\subsection{Combined Models}
As seen in the correlation matrix (Figure \ref{fig:correlation_matrix}) \textit{Model 1} and \textit{Model 2} correlate the most with a correlation value of 0.589. The same feature input explains this behaviour. The correlation between \textit{Model 3} and both \textit{Model 1} and \textit{Model 2} is also relatively high with the values $0.289$ and $0.255$ respectively. A reason for this could be that a headline and the first 50 words of an article transport a very similar context.

\textit{Model 2} is the best model using text therefore we decided to combine it with each other model. For most other models the correlation with \textit{Model 2} is low which also justifies this decision.
\textit{Model 5} which is correlating least with all other models has a very low $F_1$ score. Therefore we excluded it from our consideration to combine it with every other model.

The models, that we created combining \textit{Model 2} with each \textit{Model 5}, \textit{6} and \textit{7} performed almost the same as \textit{Model 2} itself. Therefore, we figured that publishing time, text metrics and competitive score are unqualified features for the headline text.

Combining each \textit{Model 3} and \textit{Model 4} with \textit{Model 2} showed small improvements in performance. In consequence we combined \textit{model 2}, \textit{Model 3} and \textit{Model 4} and got our best results. The better $F_1$-score of $0.357$ was due to the improvements of the precision value and a consistent recall.

Our $F_1$-score was $37\%$ higher than the presented value from Tsagikas et al. \cite{tsagkias2009predicting} and $24\%$ smaller than the presented value from Ambroselli et al. \cite{ambroselli2018prediction}.

BEGRÃœNDUNG \& RECHTFERTIGUNG

\input{combined_results.tex}