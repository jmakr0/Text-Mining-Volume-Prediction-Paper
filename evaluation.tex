We use \textit{precision} and \textit{recall} to evaluate our models. 
In practice, each article which produces a high amount of comments is more likely to bind resources of comment moderators and editors. Therefore, a high classification precision is important as well as a high recall to omit as few as possible.
Precision and recall are equally relevant and consequently, we use the harmonic mean, the $F_1$-score, of both metrics. As \textit{precision}, \textit{recall}, and the $F_1$-score are common metrics for binary classification problems, we are enabled to compare our results against existing approaches.

\subsection{Model Performances}
The overview of our results in \autoref{tbl:results_basic} depicts that all our models reach a significantly higher \textit{recall} than \textit{precision}.

\textit{Model 3} reaches the highest \textit{precision} with a value of $0.228$, and \textit{Model 7} the highest \textit{recall} with $0.917$. \textit{Model 4} is the best overall performing model with an $F_1$-score of $0.320$. 
%\textit{Model 1} to \textit{3} also performs relatively well with an $F_1$-scores of over $0.3$.

\textit{Model 1} and \textit{Model 2} process the same input. Despite the fact that the second model has approximately a third of the weights in its hidden layers, both models exhibit a similar performance for all three metrics.
We assume that the convolutional network can learn fewer features but has a much higher generalization potential through its shared weights.

\textit{Model 3} takes the first $50$ words of the article text as input. We discovered that using more or even all words has no positive effect on the model's predictions.

Compared to \textit{Model 1} and \textit{2} it reaches a worse \textit{recall} but a slightly higher \textit{precision} which leads to a similar $F_1$-score.
We suppose this occurs because of a similar context complexity which is provided by the headline and the first words of the article.

\textit{Model 4} achieves the best results. This seems surprising due to the low complexity of this feature but the relatively good results can be justified with the distribution of the categories within the GACC. 

\begin{figure}
	\includegraphics[width=0.45\textwidth]{fig/top_ten_category.png}
	\caption{\textmd{bla }}
	\label{fig:top_ten_category}
\end{figure}

Articles assigned to the most used category have a probability of $22.8\%$ to be a top article. Classifying just these articles as a top article already leads to an $F_1$-score of $0.292$. 
For instance, if we only classify articles within the four categories with the highest amount of top articles as a top article, we receive an $F_1$-score of $0.316$ which is almost as good as the result achieved by this model.

\textit{Model 5} is the worst performing model. Its precision of $0.100$ is just as good as a random prediction.

\begin{figure}
	\includegraphics[width=0.45\textwidth]{fig/top_ten_time.png}
	\caption{\textmd{bla }}
	\label{fig:top_ten_time}
\end{figure}

One reason is that $19\%$ of all articles got released on full hours and might also be due to the fact that \textit{The Guardian} has an international readership spread over multiple time zones. Additionally, the day of the week has no significant effect on the weekly article performance. Therefore, this feature has almost no effect on the number of comments that an article receives. 

\textit{Model 6} had also a small but slightly better performance than \textit{Model 5}. 
Either the length of the headline or the length of the article text is feasible for an accurate prediction.

\textit{Model 7} processes just a single numeric value as input. For a good performance using the competitive score it would be necessary to be able to divide the values into distinct intervals that can be assigned to each class.
Unfortunately, the competitive score distribution of top articles is similar to the overall distribution of the competitive score.

\input{basic_results.tex}

\subsection{Combined Models}
As seen in \autoref{fig:correlation_matrix}, \textit{Model 1} and \textit{2} correlate the most with a value of $0.589$ which is due to processing the same input feature.
The correlation between \textit{Model 3} and both \textit{Model 1} and \textit{2} is also relatively high because the headline and the first $50$ words of the article provide a similar context.

We combine \textit{Model 2} with each other model because it exhibits the best results processing text input and has a low correlation to other models.
\textit{Model 5} which is correlating least with all other models has a very low $F_1$ score. Therefore, we exclude it from our consideration to combine it with every other model.

The combination of \textit{Model 2} with each \textit{Model 5}, \textit{6}, and \textit{7} doesn't have a better performance than \textit{Model 2} itself. 
Thus, we notice that the publishing time, text metrics, and competitive score are unqualified additional features for the headline text.

Combining each \textit{Model 3} and \textit{4} with \textit{Model 2} shows performance improvements. Consequently, we combine these models and receive our best \textit{precision} and $F_1$-score.

Compared to other approaches, our $F_1$-score is $37\%$ higher than the presented value from Tsagikas et al. \cite{tsagkias2009predicting} and $24\%$ smaller than the values from Ambroselli et al. \cite{ambroselli2018prediction}. 

- other dataset
- other user behavior
	- one nation
	- less diverse
	- readership uniform

%The German audience acts more commonly than a international readership.
%Wetterdaten fÃ¼r deutschland konstant -> International schwieriger

\input{combined_results.tex}