\subsection{Net Architecture}

We wanted to evaluate the quality of basic features. For some features we tried multiple different architectures to compare them among themselves. That resulted in seven \textit{basic models} (BM). Each of the models has an ID for referencing within the further text.
All models have single or related features as input and generate a binary classification output using sigmoid as our activation function.

\paragraph{BM 1} This model takes the article headline as input. We normalized the headline length and embedded the words using an embedding layer. This we initialized with pretrainied glove embeddings (reference). The model uses a dense and batch normalization layer as hidden layers.
\paragraph{BM 2} Like BM 1 this model takes the article headline as input. Instead of a dense layer this one uses a convolutional layer with kernel sizes one, tree and five as well as a pooling layer as proposed by (hier name von kim) (reference).
\paragraph{BM 3} This model takes the first words of the article text. They get embedded the same way as in BM 1 and BM 2 and processed through an LSTM layer outputting the last cell state.
\paragraph{BM 4} This model takes the article category as input. The category id gets embedded using an embedding layer and processed through a dense and a batch normalization layer.
\paragraph{BM 5} This model takes temporal features of the publication as input. The features are minute, hour, day of the week and day of the year. The features get processed the same way as in BM 4.
\paragraph{BM 6} This model takes the headline and the article word count. From both of them the logarithm is calculated. The logarithm is used to create exponential sized bins for different lengths. Each logarithm gets embedded and processed like in BM 5 and 6.
\paragraph{BM 7} This model takes the competitive score (link) and processes it through a dense layer as well as batch normalization.

\subsection{Training}
We split the dataset in training, validation and test sets using a 70\%/15\%/15\% split with respect to the timestamps of publication. We chose this split because in a real world scenario the whole set would be used to train the model and predict on articles in the future (quote julian).
Because our task implies a strongly imbalanced training set we used class weights. They influence the weighting of the loss while training indirectly proportional to the class size of a giving training sample.
